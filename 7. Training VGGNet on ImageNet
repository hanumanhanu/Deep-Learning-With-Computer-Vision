7.1 Implementing VGGNet

Therefore, whenever you train VGG16 or VGG19 make sure you:
1. Swap out all ReLUs for PReLUs.
2. Use MSRA (also called “He et al. initialization”) to initialize the weight layers in your
network.

--- pyimagesearch
| |--- __init__.py
| |--- callbacks
| |--- nn
| | |--- __init__.py
| | |--- conv
| | |--- mxconv
| | | |--- __init__.py
| | | |--- mxalexnet.py
| | | |--- mxvggnet.py
| |--- preprocessing
| |--- utils

# import the necessary packages
import mxnet as mx
class MxVGGNet:
@staticmethod
def build(classes):
# data input
data = mx.sym.Variable("data")
# Block #1: (CONV => RELU) * 2 => POOL
 conv1_1 = mx.sym.Convolution(data=data, kernel=(3, 3),pad=(1, 1), num_filter=64, name="conv1_1")
 act1_1 = mx.sym.LeakyReLU(data=conv1_1, act_type="prelu",name="act1_1")
 bn1_1 = mx.sym.BatchNorm(data=act1_1, name="bn1_1")
conv1_2 = mx.sym.Convolution(data=bn1_1, kernel=(3, 3),pad=(1, 1), num_filter=64, name="conv1_2")
act1_2 = mx.sym.LeakyReLU(data=conv1_2, act_type="prelu",name="act1_2")
bn1_2 = mx.sym.BatchNorm(data=act1_2, name="bn1_2")
pool1 = mx.sym.Pooling(data=bn1_2, pool_type="max",kernel=(2, 2), stride=(2, 2), name="pool1")
do1 = mx.sym.Dropout(data=pool1, p=0.25)
# Block #2: (CONV => RELU) * 2 => POOL
conv2_1 = mx.sym.Convolution(data=do1, kernel=(3, 3),pad=(1, 1), num_filter=128, name="conv2_1")
act2_1 = mx.sym.LeakyReLU(data=conv2_1, act_type="prelu",name="act2_1")
bn2_1 = mx.sym.BatchNorm(data=act2_1, name="bn2_1")
conv2_2 = mx.sym.Convolution(data=bn2_1, kernel=(3, 3),
pad=(1, 1), num_filter=128, name="conv2_2")
act2_2 = mx.sym.LeakyReLU(data=conv2_2, act_type="prelu",name="act2_2")
bn2_2 = mx.sym.BatchNorm(data=act2_2, name="bn2_2")
pool2 = mx.sym.Pooling(data=bn2_2, pool_type="max",kernel=(2, 2), stride=(2, 2), name="pool2")
do2 = mx.sym.Dropout(data=pool2, p=0.25)
# Block #3: (CONV => RELU) * 3 => POOL
conv3_1 = mx.sym.Convolution(data=do2, kernel=(3, 3), pad=(1, 1), num_filter=256, name="conv3_1")
act3_1 = mx.sym.LeakyReLU(data=conv3_1, act_type="prelu",name="act3_1")
bn3_1 = mx.sym.BatchNorm(data=act3_1, name="bn3_1")
conv3_2 = mx.sym.Convolution(data=bn3_1, kernel=(3, 3),pad=(1, 1), num_filter=256, name="conv3_2")
act3_2 = mx.sym.LeakyReLU(data=conv3_2, act_type="prelu",name="act3_2")
bn3_2 = mx.sym.BatchNorm(data=act3_2, name="bn3_2")
conv3_3 = mx.sym.Convolution(data=bn3_2, kernel=(3, 3),pad=(1, 1), num_filter=256, name="conv3_3")
act3_3 = mx.sym.LeakyReLU(data=conv3_3, act_type="prelu",name="act3_3")
bn3_3 = mx.sym.BatchNorm(data=act3_3, name="bn3_3")
pool3 = mx.sym.Pooling(data=bn3_3, pool_type="max",kernel=(2, 2), stride=(2, 2), name="pool3")
do3 = mx.sym.Dropout(data=pool3, p=0.25)
# Block #4: (CONV => RELU) * 3 => POOL
conv4_1 = mx.sym.Convolution(data=do3, kernel=(3, 3),pad=(1, 1), num_filter=512, name="conv4_1")
act4_1 = mx.sym.LeakyReLU(data=conv4_1, act_type="prelu",name="act4_1")
bn4_1 = mx.sym.BatchNorm(data=act4_1, name="bn4_1")
conv4_2 = mx.sym.Convolution(data=bn4_1, kernel=(3, 3),pad=(1, 1), num_filter=512, name="conv4_2")
act4_2 = mx.sym.LeakyReLU(data=conv4_2, act_type="prelu",name="act4_2")
bn4_2 = mx.sym.BatchNorm(data=act4_2, name="bn4_2")
conv4_3 = mx.sym.Convolution(data=bn4_2, kernel=(3, 3),
pad=(1, 1), num_filter=512, name="conv4_3")
act4_3 = mx.sym.LeakyReLU(data=conv4_3, act_type="prelu",name="act4_3")
bn4_3 = mx.sym.BatchNorm(data=act4_3, name="bn4_3")
pool4 = mx.sym.Pooling(data=bn4_3, pool_type="max",kernel=(2, 2), stride=(2, 2), name="pool3")
do4 = mx.sym.Dropout(data=pool4, p=0.25)
# Block #5: (CONV => RELU) * 3 => POOL
conv5_1 = mx.sym.Convolution(data=do4, kernel=(3, 3),
pad=(1, 1), num_filter=512, name="conv5_1")
act5_1 = mx.sym.LeakyReLU(data=conv5_1, act_type="prelu",name="act5_1")
bn5_1 = mx.sym.BatchNorm(data=act5_1, name="bn5_1")
conv5_2 = mx.sym.Convolution(data=bn5_1, kernel=(3, 3),pad=(1, 1), num_filter=512, name="conv5_2")
act5_2 = mx.sym.LeakyReLU(data=conv5_2, act_type="prelu",name="act5_2")
bn5_2 = mx.sym.BatchNorm(data=act5_2, name="bn5_2")
conv5_3 = mx.sym.Convolution(data=bn5_2, kernel=(3, 3),pad=(1, 1), num_filter=512, name="conv5_3")
act5_3 = mx.sym.LeakyReLU(data=conv5_3, act_type="prelu",name="act5_3")
bn5_3 = mx.sym.BatchNorm(data=act5_3, name="bn5_3")
pool5 = mx.sym.Pooling(data=bn5_3, pool_type="max",kernel=(2, 2), stride=(2, 2), name="pool5")
do5 = mx.sym.Dropout(data=pool5, p=0.25)
 # Block #6: FC => RELU layers
flatten = mx.sym.Flatten(data=do5, name="flatten")
fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=4096,name="fc1")
act6_1 = mx.sym.LeakyReLU(data=fc1, act_type="prelu",name="act6_1")
bn6_1 = mx.sym.BatchNorm(data=act6_1, name="bn6_1")
do6 = mx.sym.Dropout(data=bn6_1, p=0.5)
# Block #7: FC => RELU layers
fc2 = mx.sym.FullyConnected(data=do6, num_hidden=4096,name="fc2")
act7_1 = mx.sym.LeakyReLU(data=fc2, act_type="prelu",
name="act7_1")
bn7_1 = mx.sym.BatchNorm(data=act7_1, name="bn7_1")
do7 = mx.sym.Dropout(data=bn7_1, p=0.5)
# softmax classifier
fc3 = mx.sym.FullyConnected(data=do7, num_hidden=classes,name="fc3")
model = mx.sym.SoftmaxOutput(data=fc3, name="softmax")
# return the network architecture
return model

7.2 Training VGGNet

--- mx_imagenet_vggnet
| |--- config
| | |--- __init__.py
| | |--- imagnet_vggnet_config.py
| |--- output/
| |--- test_vggnet.py
| |--- train_vggnet.py
 # import the necessary packages
from config import imagenet_vggnet_config as config
from pyimagesearch.nn.mxconv import MxVGGNet
import mxnet as mx
import argparse
import logging
import json
import os
 # construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--checkpoints", required=True,help="path to output checkpoint directory")
ap.add_argument("-p", "--prefix", required=True,help="name of model prefix")
ap.add_argument("-s", "--start-epoch", type=int, default=0,help="epoch to restart training at")
args = vars(ap.parse_args())
# set the logging level and output file
logging.basicConfig(level=logging.DEBUG,filename="training_{}.log".format(args["start_epoch"]),filemode="w")
# load the RGB means for the training set, then determine the batch
# size
means = json.loads(open(config.DATASET_MEAN).read())
batchSize = config.BATCH_SIZE * config.NUM_DEVICES
# construct the training image iterator
 trainIter = mx.io.ImageRecordIter(path_imgrec=config.TRAIN_MX_REC,data_shape=(3, 224, 224),batch_size=batchSize,rand_crop=True,rand_mirror=True,rotate=15,max_shear_ratio=0.1,mean_r=means["R"],mean_g=means["G"],mean_b=means["B"],preprocess_threads=config.NUM_DEVICES * 2)
# construct the validation image iterator
valIter = mx.io.ImageRecordIter(path_imgrec=config.VAL_MX_REC,data_shape=(3, 224, 224),batch_size=batchSize,mean_r=means["R"],mean_g=means["G"],mean_b=means["B"])
 # initialize the optimizer
opt = mx.optimizer.SGD(learning_rate=1e-2, momentum=0.9, wd=0.0005,rescale_grad=1.0 / batchSize)
# construct the checkpoints path, initialize the model argument and
# auxiliary parameters
checkpointsPath = os.path.sep.join([args["checkpoints"],args["prefix"]])
argParams = None
auxParams = None
# if there is no specific model starting epoch supplied, then
# initialize the network
if args["start_epoch"] <= 0:
# build the LeNet architecture
print("[INFO] building network...")
model = MxVGGNet.build(config.NUM_CLASSES)

# otherwise, a specific checkpoint was supplied
else:
# load the checkpoint from disk
print("[INFO] loading epoch {}...".format(args["start_epoch"]))
model = mx.model.FeedForward.load(checkpointsPath,
args["start_epoch"])
# update the model and parameters
argParams = model.arg_params
auxParams = model.aux_params
model = model.symbol
 # compile the model
model = mx.model.FeedForward(ctx=[mx.gpu(i) for i in range(0, config.NUM_DEVICES)],symbol=model,initializer=mx.initializer.MSRAPrelu(),arg_params=argParams,aux_params=auxParams,optimizer=opt,num_epoch=80,begin_epoch=args["start_epoch"])
# initialize the callbacks and evaluation metrics
 batchEndCBs = [mx.callback.Speedometer(batchSize, 250)]
epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]
metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5),
mx.metric.CrossEntropy()]
 # train the network
 print("[INFO] training network...")
 model.fit(X=trainIter,eval_data=valIter,eval_metric=metrics,batch_end_callback=batchEndCBs,epoch_end_callback=epochEndCBs)

