7.1 Implementing VGGNet

Therefore, whenever you train VGG16 or VGG19 make sure you:
1. Swap out all ReLUs for PReLUs.
2. Use MSRA (also called “He et al. initialization”) to initialize the weight layers in your
network.

--- pyimagesearch
| |--- __init__.py
| |--- callbacks
| |--- nn
| | |--- __init__.py
| | |--- conv
| | |--- mxconv
| | | |--- __init__.py
| | | |--- mxalexnet.py
| | | |--- mxvggnet.py
| |--- preprocessing
| |--- utils

1 # import the necessary packages
2 import mxnet as mx
3
4 class MxVGGNet:
5 @staticmethod
6 def build(classes):
7 # data input
8 data = mx.sym.Variable("data")
10 # Block #1: (CONV => RELU) * 2 => POOL
11 conv1_1 = mx.sym.Convolution(data=data, kernel=(3, 3),
12 pad=(1, 1), num_filter=64, name="conv1_1")
13 act1_1 = mx.sym.LeakyReLU(data=conv1_1, act_type="prelu",
14 name="act1_1")
15 bn1_1 = mx.sym.BatchNorm(data=act1_1, name="bn1_1")
16 conv1_2 = mx.sym.Convolution(data=bn1_1, kernel=(3, 3),
17 pad=(1, 1), num_filter=64, name="conv1_2")
18 act1_2 = mx.sym.LeakyReLU(data=conv1_2, act_type="prelu",
19 name="act1_2")
20 bn1_2 = mx.sym.BatchNorm(data=act1_2, name="bn1_2")
21 pool1 = mx.sym.Pooling(data=bn1_2, pool_type="max",
22 kernel=(2, 2), stride=(2, 2), name="pool1")
23 do1 = mx.sym.Dropout(data=pool1, p=0.25)
25 # Block #2: (CONV => RELU) * 2 => POOL
26 conv2_1 = mx.sym.Convolution(data=do1, kernel=(3, 3),
27 pad=(1, 1), num_filter=128, name="conv2_1")
28 act2_1 = mx.sym.LeakyReLU(data=conv2_1, act_type="prelu",
29 name="act2_1")
30 bn2_1 = mx.sym.BatchNorm(data=act2_1, name="bn2_1")
31 conv2_2 = mx.sym.Convolution(data=bn2_1, kernel=(3, 3),
32 pad=(1, 1), num_filter=128, name="conv2_2")
33 act2_2 = mx.sym.LeakyReLU(data=conv2_2, act_type="prelu",
34 name="act2_2")
35 bn2_2 = mx.sym.BatchNorm(data=act2_2, name="bn2_2")
36 pool2 = mx.sym.Pooling(data=bn2_2, pool_type="max",
37 kernel=(2, 2), stride=(2, 2), name="pool2")
38 do2 = mx.sym.Dropout(data=pool2, p=0.25)
40 # Block #3: (CONV => RELU) * 3 => POOL
41 conv3_1 = mx.sym.Convolution(data=do2, kernel=(3, 3),
42 pad=(1, 1), num_filter=256, name="conv3_1")
43 act3_1 = mx.sym.LeakyReLU(data=conv3_1, act_type="prelu",
44 name="act3_1")
45 bn3_1 = mx.sym.BatchNorm(data=act3_1, name="bn3_1")
46 conv3_2 = mx.sym.Convolution(data=bn3_1, kernel=(3, 3),
47 pad=(1, 1), num_filter=256, name="conv3_2")
48 act3_2 = mx.sym.LeakyReLU(data=conv3_2, act_type="prelu",
49 name="act3_2")
50 bn3_2 = mx.sym.BatchNorm(data=act3_2, name="bn3_2")
51 conv3_3 = mx.sym.Convolution(data=bn3_2, kernel=(3, 3),
52 pad=(1, 1), num_filter=256, name="conv3_3")
53 act3_3 = mx.sym.LeakyReLU(data=conv3_3, act_type="prelu",
54 name="act3_3")
55 bn3_3 = mx.sym.BatchNorm(data=act3_3, name="bn3_3")
56 pool3 = mx.sym.Pooling(data=bn3_3, pool_type="max",
57 kernel=(2, 2), stride=(2, 2), name="pool3")
58 do3 = mx.sym.Dropout(data=pool3, p=0.25)
60 # Block #4: (CONV => RELU) * 3 => POOL
61 conv4_1 = mx.sym.Convolution(data=do3, kernel=(3, 3),
62 pad=(1, 1), num_filter=512, name="conv4_1")
63 act4_1 = mx.sym.LeakyReLU(data=conv4_1, act_type="prelu",
64 name="act4_1")
65 bn4_1 = mx.sym.BatchNorm(data=act4_1, name="bn4_1")
66 conv4_2 = mx.sym.Convolution(data=bn4_1, kernel=(3, 3),
67 pad=(1, 1), num_filter=512, name="conv4_2")
68 act4_2 = mx.sym.LeakyReLU(data=conv4_2, act_type="prelu",
69 name="act4_2")
70 bn4_2 = mx.sym.BatchNorm(data=act4_2, name="bn4_2")
71 conv4_3 = mx.sym.Convolution(data=bn4_2, kernel=(3, 3),
72 pad=(1, 1), num_filter=512, name="conv4_3")
73 act4_3 = mx.sym.LeakyReLU(data=conv4_3, act_type="prelu",
74 name="act4_3")
75 bn4_3 = mx.sym.BatchNorm(data=act4_3, name="bn4_3")
76 pool4 = mx.sym.Pooling(data=bn4_3, pool_type="max",
77 kernel=(2, 2), stride=(2, 2), name="pool3")
78 do4 = mx.sym.Dropout(data=pool4, p=0.25)
80 # Block #5: (CONV => RELU) * 3 => POOL
81 conv5_1 = mx.sym.Convolution(data=do4, kernel=(3, 3),
82 pad=(1, 1), num_filter=512, name="conv5_1")
83 act5_1 = mx.sym.LeakyReLU(data=conv5_1, act_type="prelu",
84 name="act5_1")
85 bn5_1 = mx.sym.BatchNorm(data=act5_1, name="bn5_1")
86 conv5_2 = mx.sym.Convolution(data=bn5_1, kernel=(3, 3),
87 pad=(1, 1), num_filter=512, name="conv5_2")
88 act5_2 = mx.sym.LeakyReLU(data=conv5_2, act_type="prelu",
89 name="act5_2")
90 bn5_2 = mx.sym.BatchNorm(data=act5_2, name="bn5_2")
91 conv5_3 = mx.sym.Convolution(data=bn5_2, kernel=(3, 3),
92 pad=(1, 1), num_filter=512, name="conv5_3")
93 act5_3 = mx.sym.LeakyReLU(data=conv5_3, act_type="prelu",
94 name="act5_3")
95 bn5_3 = mx.sym.BatchNorm(data=act5_3, name="bn5_3")
96 pool5 = mx.sym.Pooling(data=bn5_3, pool_type="max",
97 kernel=(2, 2), stride=(2, 2), name="pool5")
98 do5 = mx.sym.Dropout(data=pool5, p=0.25)
100 # Block #6: FC => RELU layers
101 flatten = mx.sym.Flatten(data=do5, name="flatten")
102 fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=4096,
103 name="fc1")
104 act6_1 = mx.sym.LeakyReLU(data=fc1, act_type="prelu",
105 name="act6_1")
106 bn6_1 = mx.sym.BatchNorm(data=act6_1, name="bn6_1")
107 do6 = mx.sym.Dropout(data=bn6_1, p=0.5)
109 # Block #7: FC => RELU layers
110 fc2 = mx.sym.FullyConnected(data=do6, num_hidden=4096,
111 name="fc2")
112 act7_1 = mx.sym.LeakyReLU(data=fc2, act_type="prelu",
113 name="act7_1")
114 bn7_1 = mx.sym.BatchNorm(data=act7_1, name="bn7_1")
115 do7 = mx.sym.Dropout(data=bn7_1, p=0.5)
117 # softmax classifier
118 fc3 = mx.sym.FullyConnected(data=do7, num_hidden=classes,
119 name="fc3")
120 model = mx.sym.SoftmaxOutput(data=fc3, name="softmax")
121
122 # return the network architecture
123 return model

7.2 Training VGGNet

--- mx_imagenet_vggnet
| |--- config
| | |--- __init__.py
| | |--- imagnet_vggnet_config.py
| |--- output/
| |--- test_vggnet.py
| |--- train_vggnet.py

1 # import the necessary packages
2 from config import imagenet_vggnet_config as config
3 from pyimagesearch.nn.mxconv import MxVGGNet
4 import mxnet as mx
5 import argparse
6 import logging
7 import json
8 import os
10 # construct the argument parse and parse the arguments
11 ap = argparse.ArgumentParser()
12 ap.add_argument("-c", "--checkpoints", required=True,
13 help="path to output checkpoint directory")
14 ap.add_argument("-p", "--prefix", required=True,
15 help="name of model prefix")
16 ap.add_argument("-s", "--start-epoch", type=int, default=0,
17 help="epoch to restart training at")
18 args = vars(ap.parse_args())
19
20 # set the logging level and output file
21 logging.basicConfig(level=logging.DEBUG,
22 filename="training_{}.log".format(args["start_epoch"]),
23 filemode="w")
24
25 # load the RGB means for the training set, then determine the batch
26 # size
27 means = json.loads(open(config.DATASET_MEAN).read())
28 batchSize = config.BATCH_SIZE * config.NUM_DEVICES
30 # construct the training image iterator
31 trainIter = mx.io.ImageRecordIter(
32 path_imgrec=config.TRAIN_MX_REC,
33 data_shape=(3, 224, 224),
34 batch_size=batchSize,
35 rand_crop=True,
36 rand_mirror=True,
37 rotate=15,
38 max_shear_ratio=0.1,
39 mean_r=means["R"],
40 mean_g=means["G"],
41 mean_b=means["B"],
42 preprocess_threads=config.NUM_DEVICES * 2)
44 # construct the validation image iterator
45 valIter = mx.io.ImageRecordIter(
46 path_imgrec=config.VAL_MX_REC,
47 data_shape=(3, 224, 224),
48 batch_size=batchSize,
49 mean_r=means["R"],
50 mean_g=means["G"],
51 mean_b=means["B"])
53 # initialize the optimizer
54 opt = mx.optimizer.SGD(learning_rate=1e-2, momentum=0.9, wd=0.0005,
55 rescale_grad=1.0 / batchSize)
57 # construct the checkpoints path, initialize the model argument and
58 # auxiliary parameters
59 checkpointsPath = os.path.sep.join([args["checkpoints"],
60 args["prefix"]])
61 argParams = None
62 auxParams = None
64 # if there is no specific model starting epoch supplied, then
65 # initialize the network
66 if args["start_epoch"] <= 0:
67 # build the LeNet architecture
68 print("[INFO] building network...")
69 model = MxVGGNet.build(config.NUM_CLASSES)
70
71 # otherwise, a specific checkpoint was supplied
72 else:
73 # load the checkpoint from disk
74 print("[INFO] loading epoch {}...".format(args["start_epoch"]))
75 model = mx.model.FeedForward.load(checkpointsPath,
76 args["start_epoch"])
77
78 # update the model and parameters
79 argParams = model.arg_params
80 auxParams = model.aux_params
81 model = model.symbol
83 # compile the model
84 model = mx.model.FeedForward(
85 ctx=[mx.gpu(i) for i in range(0, config.NUM_DEVICES)],
86 symbol=model,
87 initializer=mx.initializer.MSRAPrelu(),
88 arg_params=argParams,
89 aux_params=auxParams,
90 optimizer=opt,
91 num_epoch=80,
92 begin_epoch=args["start_epoch"])
94 # initialize the callbacks and evaluation metrics
95 batchEndCBs = [mx.callback.Speedometer(batchSize, 250)]
96 epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]
97 metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5),
98 mx.metric.CrossEntropy()]
100 # train the network
101 print("[INFO] training network...")
102 model.fit(
103 X=trainIter,
104 eval_data=valIter,
105 eval_metric=metrics,
106 batch_end_callback=batchEndCBs,
107 epoch_end_callback=epochEndCBs)

