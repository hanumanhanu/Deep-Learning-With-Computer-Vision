6.1 Implementing AlexNet

1 # import the necessary packages
2 import mxnet as mx
3
4 class MxAlexNet:
5 @staticmethod
6 def build(classes):
7 # data input
8 data = mx.sym.Variable("data")
10 # Block #1: first CONV => RELU => POOL layer set
11 conv1_1 = mx.sym.Convolution(data=data, kernel=(11, 11),
12 stride=(4, 4), num_filter=96)
13 act1_1 = mx.sym.LeakyReLU(data=conv1_1, act_type="elu")
14 bn1_1 = mx.sym.BatchNorm(data=act1_1)
15 pool1 = mx.sym.Pooling(data=bn1_1, pool_type="max",
16 kernel=(3, 3), stride=(2, 2))
17 do1 = mx.sym.Dropout(data=pool1, p=0.25)
19 # Block #2: second CONV => RELU => POOL layer set
20 conv2_1 = mx.sym.Convolution(data=do1, kernel=(5, 5),
21 pad=(2, 2), num_filter=256)
22 act2_1 = mx.sym.LeakyReLU(data=conv2_1, act_type="elu")
23 bn2_1 = mx.sym.BatchNorm(data=act2_1)
24 pool2 = mx.sym.Pooling(data=bn2_1, pool_type="max",
25 kernel=(3, 3), stride=(2, 2))
26 do2 = mx.sym.Dropout(data=pool2, p=0.25)
28 # Block #3: (CONV => RELU) * 3 => POOL
29 conv3_1 = mx.sym.Convolution(data=do2, kernel=(3, 3),
30 pad=(1, 1), num_filter=384)
31 act3_1 = mx.sym.LeakyReLU(data=conv3_1, act_type="elu")
32 bn3_1 = mx.sym.BatchNorm(data=act3_1)
33 conv3_2 = mx.sym.Convolution(data=bn3_1, kernel=(3, 3),
34 pad=(1, 1), num_filter=384)
35 act3_2 = mx.sym.LeakyReLU(data=conv3_2, act_type="elu")
36 bn3_2 = mx.sym.BatchNorm(data=act3_2)
37 conv3_3 = mx.sym.Convolution(data=bn3_2, kernel=(3, 3),
38 pad=(1, 1), num_filter=256)
39 act3_3 = mx.sym.LeakyReLU(data=conv3_3, act_type="elu")
40 bn3_3 = mx.sym.BatchNorm(data=act3_3)
41 pool3 = mx.sym.Pooling(data=bn3_3, pool_type="max",
42 kernel=(3, 3), stride=(2, 2))
43 do3 = mx.sym.Dropout(data=pool3, p=0.25)
45 # Block #4: first set of FC => RELU layers
46 flatten = mx.sym.Flatten(data=do3)
47 fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=4096)
48 act4_1 = mx.sym.LeakyReLU(data=fc1, act_type="elu")
49 bn4_1 = mx.sym.BatchNorm(data=act4_1)
50 do4 = mx.sym.Dropout(data=bn4_1, p=0.5)
51
52 # Block #5: second set of FC => RELU layers
53 fc2 = mx.sym.FullyConnected(data=do4, num_hidden=4096)
54 act5_1 = mx.sym.LeakyReLU(data=fc2, act_type="elu")
55 bn5_1 = mx.sym.BatchNorm(data=act5_1)
56 do5 = mx.sym.Dropout(data=bn5_1, p=0.5)
58 # softmax classifier
59 fc3 = mx.sym.FullyConnected(data=do5, num_hidden=classes)
60 model = mx.sym.SoftmaxOutput(data=fc3, name="softmax")
61
62 # return the network architecture
63 return model

6.2 Training AlexNet

two key differences:
1. The mxnet training code is slightly more verbose due to the fact that we would like to leverage
multiple GPUs (if possible).
2. The mxnet library doesnâ€™t provide a convenient method to plot our loss/accuracy over time
and instead logs training progress to the terminal.

6.2.2 Implementing the Training Script

from config import imagenet_alexnet_config as config
from pyimagesearch.nn.mxconv import MxAlexNet
import mxnet as mx
import argparse
import logging
import json
import os
 # construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--checkpoints", required=True,
help="path to output checkpoint directory")
ap.add_argument("-p", "--prefix", required=True,help="name of model prefix")
ap.add_argument("-s", "--start-epoch", type=int, default=0,help="epoch to restart training at")
args = vars(ap.parse_args())
# set the logging level and output file
 logging.basicConfig(level=logging.DEBUG,
filename="training_{}.log".format(args["start_epoch"]), filemode="w")
# load the RGB means for the training set, then determine the batch
# size
means = json.loads(open(config.DATASET_MEAN).read())
batchSize = config.BATCH_SIZE * config.NUM_DEVICES
# construct the training image iterator
trainIter = mx.io.ImageRecordIter(path_imgrec=config.TRAIN_MX_REC,data_shape=(3, 227, 227),batch_size=batchSize,
rand_crop=True,rand_mirror=True,rotate=15,max_shear_ratio=0.1,mean_r=means["R"],mean_g=means["G"],mean_b=means["B"],
preprocess_threads=config.NUM_DEVICES * 2)
# construct the validation image iterator
valIter = mx.io.ImageRecordIter(path_imgrec=config.VAL_MX_REC,data_shape=(3, 227, 227),batch_size=batchSize,mean_r=means["R"],mean_g=means["G"],mean_b=means["B"])
# initialize the optimizer
opt = mx.optimizer.SGD(learning_rate=1e-2, momentum=0.9, wd=0.0005,rescale_grad=1.0 / batchSize)
# construct the checkpoints path, initialize the model argument and
# auxiliary parameters
checkpointsPath = os.path.sep.join([args["checkpoints"],args["prefix"]])
argParams = None
 # if there is no specific model starting epoch supplied, then
 # initialize the network
 if args["start_epoch"] <= 0:
 # build the LeNet architecture
print("[INFO] building network...")
model = MxAlexNet.build(config.NUM_CLASSES)
 # otherwise, a specific checkpoint was supplied
  else:
# load the checkpoint from disk
print("[INFO] loading epoch {}...".format(args["start_epoch"]))
model = mx.model.FeedForward.load(checkpointsPath,args["start_epoch"])

 # update the model and parameters
argParams = model.arg_params
auxParams = model.aux_params
model = model.symbol
# compile the model
model = mx.model.FeedForward(
ctx=[mx.gpu(1), mx.gpu(2), mx.gpu(3)],
symbol=model,
initializer=mx.initializer.Xavier(),
arg_params=argParams,
aux_params=auxParams,
optimizer=opt,
num_epoch=90,
begin_epoch=args["start_epoch"])
# initialize the callbacks and evaluation metrics
batchEndCBs = [mx.callback.Speedometer(batchSize, 500)]
epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]
metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5),
mx.metric.CrossEntropy()]
# train the network
print("[INFO] training network...")
model.fit(X=trainIter,eval_data=valIter,eval_metric=metrics,batch_end_callback=batchEndCBs,epoch_end_callback=epochEndCBs)


6.3 Evaluating AlexNet

1 # import the necessary packages
2 from config import imagenet_alexnet_config as config
3 import mxnet as mx
4 import argparse
5 import json
6 import os
8 # construct the argument parse and parse the arguments
9 ap = argparse.ArgumentParser()
10 ap.add_argument("-c", "--checkpoints", required=True,
11 help="path to output checkpoint directory")
12 ap.add_argument("-p", "--prefix", required=True,
13 help="name of model prefix")
14 ap.add_argument("-e", "--epoch", type=int, required=True,
15 help="epoch # to load")
16 args = vars(ap.parse_args())
18 # load the RGB means for the training set
19 means = json.loads(open(config.DATASET_MEAN).read())
20
21 # construct the testing image iterator
22 testIter = mx.io.ImageRecordIter(
23 path_imgrec=config.TEST_MX_REC,
24 data_shape=(3, 227, 227),
25 batch_size=config.BATCH_SIZE,
26 mean_r=means["R"],
27 mean_g=means["G"],
28 mean_b=means["B"])
30 # load the checkpoint from disk
31 print("[INFO] loading model...")
32 checkpointsPath = os.path.sep.join([args["checkpoints"],
33 args["prefix"]])
34 model = mx.model.FeedForward.load(checkpointsPath,
35 args["epoch"])
37 # compile the model
38 model = mx.model.FeedForward(
39 ctx=[mx.gpu(0)],
40 symbol=model.symbol,
41 arg_params=model.arg_params,
42 aux_params=model.aux_params)
44 # make predictions on the testing data
45 print("[INFO] predicting on test data...")
46 metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5)]
47 (rank1, rank5) = model.score(testIter, eval_metric=metrics)
48
49 # display the rank-1 and rank-5 accuracies
50 print("[INFO] rank-1: {:.2f}%".format(rank1 * 100))
51 print("[INFO] rank-5: {:.2f}%".format(rank5 * 100))


