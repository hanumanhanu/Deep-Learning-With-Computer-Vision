6.1 Implementing AlexNet

1 # import the necessary packages
2 import mxnet as mx
3
4 class MxAlexNet:
5 @staticmethod
6 def build(classes):
7 # data input
8 data = mx.sym.Variable("data")
10 # Block #1: first CONV => RELU => POOL layer set
11 conv1_1 = mx.sym.Convolution(data=data, kernel=(11, 11),
12 stride=(4, 4), num_filter=96)
13 act1_1 = mx.sym.LeakyReLU(data=conv1_1, act_type="elu")
14 bn1_1 = mx.sym.BatchNorm(data=act1_1)
15 pool1 = mx.sym.Pooling(data=bn1_1, pool_type="max",
16 kernel=(3, 3), stride=(2, 2))
17 do1 = mx.sym.Dropout(data=pool1, p=0.25)
19 # Block #2: second CONV => RELU => POOL layer set
20 conv2_1 = mx.sym.Convolution(data=do1, kernel=(5, 5),
21 pad=(2, 2), num_filter=256)
22 act2_1 = mx.sym.LeakyReLU(data=conv2_1, act_type="elu")
23 bn2_1 = mx.sym.BatchNorm(data=act2_1)
24 pool2 = mx.sym.Pooling(data=bn2_1, pool_type="max",
25 kernel=(3, 3), stride=(2, 2))
26 do2 = mx.sym.Dropout(data=pool2, p=0.25)
28 # Block #3: (CONV => RELU) * 3 => POOL
29 conv3_1 = mx.sym.Convolution(data=do2, kernel=(3, 3),
30 pad=(1, 1), num_filter=384)
31 act3_1 = mx.sym.LeakyReLU(data=conv3_1, act_type="elu")
32 bn3_1 = mx.sym.BatchNorm(data=act3_1)
33 conv3_2 = mx.sym.Convolution(data=bn3_1, kernel=(3, 3),
34 pad=(1, 1), num_filter=384)
35 act3_2 = mx.sym.LeakyReLU(data=conv3_2, act_type="elu")
36 bn3_2 = mx.sym.BatchNorm(data=act3_2)
37 conv3_3 = mx.sym.Convolution(data=bn3_2, kernel=(3, 3),
38 pad=(1, 1), num_filter=256)
39 act3_3 = mx.sym.LeakyReLU(data=conv3_3, act_type="elu")
40 bn3_3 = mx.sym.BatchNorm(data=act3_3)
41 pool3 = mx.sym.Pooling(data=bn3_3, pool_type="max",
42 kernel=(3, 3), stride=(2, 2))
43 do3 = mx.sym.Dropout(data=pool3, p=0.25)
45 # Block #4: first set of FC => RELU layers
46 flatten = mx.sym.Flatten(data=do3)
47 fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=4096)
48 act4_1 = mx.sym.LeakyReLU(data=fc1, act_type="elu")
49 bn4_1 = mx.sym.BatchNorm(data=act4_1)
50 do4 = mx.sym.Dropout(data=bn4_1, p=0.5)
51
52 # Block #5: second set of FC => RELU layers
53 fc2 = mx.sym.FullyConnected(data=do4, num_hidden=4096)
54 act5_1 = mx.sym.LeakyReLU(data=fc2, act_type="elu")
55 bn5_1 = mx.sym.BatchNorm(data=act5_1)
56 do5 = mx.sym.Dropout(data=bn5_1, p=0.5)
58 # softmax classifier
59 fc3 = mx.sym.FullyConnected(data=do5, num_hidden=classes)
60 model = mx.sym.SoftmaxOutput(data=fc3, name="softmax")
61
62 # return the network architecture
63 return model

6.2 Training AlexNet

two key differences:
1. The mxnet training code is slightly more verbose due to the fact that we would like to leverage
multiple GPUs (if possible).
2. The mxnet library doesnâ€™t provide a convenient method to plot our loss/accuracy over time
and instead logs training progress to the terminal.

6.2.2 Implementing the Training Script

1 # import the necessary packages
2 from config import imagenet_alexnet_config as config
3 from pyimagesearch.nn.mxconv import MxAlexNet
4 import mxnet as mx
5 import argparse
6 import logging
7 import json
8 import os
10 # construct the argument parse and parse the arguments
11 ap = argparse.ArgumentParser()
12 ap.add_argument("-c", "--checkpoints", required=True,
13 help="path to output checkpoint directory")
14 ap.add_argument("-p", "--prefix", required=True,
15 help="name of model prefix")
16 ap.add_argument("-s", "--start-epoch", type=int, default=0,
17 help="epoch to restart training at")
18 args = vars(ap.parse_args())
20 # set the logging level and output file
21 logging.basicConfig(level=logging.DEBUG,
22 filename="training_{}.log".format(args["start_epoch"]),
23 filemode="w")
25 # load the RGB means for the training set, then determine the batch
26 # size
27 means = json.loads(open(config.DATASET_MEAN).read())
28 batchSize = config.BATCH_SIZE * config.NUM_DEVICES
30 # construct the training image iterator
31 trainIter = mx.io.ImageRecordIter(
32 path_imgrec=config.TRAIN_MX_REC,
33 data_shape=(3, 227, 227),
34 batch_size=batchSize,
35 rand_crop=True,
36 rand_mirror=True,
37 rotate=15,
38 max_shear_ratio=0.1,
39 mean_r=means["R"],
40 mean_g=means["G"],
41 mean_b=means["B"],
42 preprocess_threads=config.NUM_DEVICES * 2)
44 # construct the validation image iterator
45 valIter = mx.io.ImageRecordIter(
46 path_imgrec=config.VAL_MX_REC,
47 data_shape=(3, 227, 227),
48 batch_size=batchSize,
49 mean_r=means["R"],
50 mean_g=means["G"],
51 mean_b=means["B"])
53 # initialize the optimizer
54 opt = mx.optimizer.SGD(learning_rate=1e-2, momentum=0.9, wd=0.0005,
55 rescale_grad=1.0 / batchSize)
57 # construct the checkpoints path, initialize the model argument and
58 # auxiliary parameters
59 checkpointsPath = os.path.sep.join([args["checkpoints"],
60 args["prefix"]])
61 argParams = None
64 # if there is no specific model starting epoch supplied, then
65 # initialize the network
66 if args["start_epoch"] <= 0:
67 # build the LeNet architecture
68 print("[INFO] building network...")
69 model = MxAlexNet.build(config.NUM_CLASSES)
71 # otherwise, a specific checkpoint was supplied
72 else:
73 # load the checkpoint from disk
74 print("[INFO] loading epoch {}...".format(args["start_epoch"]))
75 model = mx.model.FeedForward.load(checkpointsPath,
76 args["start_epoch"])
77
78 # update the model and parameters
79 argParams = model.arg_params
80 auxParams = model.aux_params
81 model = model.symbol
83 # compile the model
84 model = mx.model.FeedForward(
85 ctx=[mx.gpu(1), mx.gpu(2), mx.gpu(3)],
86 symbol=model,
87 initializer=mx.initializer.Xavier(),
88 arg_params=argParams,
89 aux_params=auxParams,
90 optimizer=opt,
91 num_epoch=90,
92 begin_epoch=args["start_epoch"])
94 # initialize the callbacks and evaluation metrics
95 batchEndCBs = [mx.callback.Speedometer(batchSize, 500)]
96 epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]
97 metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5),
98 mx.metric.CrossEntropy()]
100 # train the network
101 print("[INFO] training network...")
102 model.fit(
103 X=trainIter,
104 eval_data=valIter,
105 eval_metric=metrics,
106 batch_end_callback=batchEndCBs,
107 epoch_end_callback=epochEndCBs)

6.3 Evaluating AlexNet

1 # import the necessary packages
2 from config import imagenet_alexnet_config as config
3 import mxnet as mx
4 import argparse
5 import json
6 import os
8 # construct the argument parse and parse the arguments
9 ap = argparse.ArgumentParser()
10 ap.add_argument("-c", "--checkpoints", required=True,
11 help="path to output checkpoint directory")
12 ap.add_argument("-p", "--prefix", required=True,
13 help="name of model prefix")
14 ap.add_argument("-e", "--epoch", type=int, required=True,
15 help="epoch # to load")
16 args = vars(ap.parse_args())
18 # load the RGB means for the training set
19 means = json.loads(open(config.DATASET_MEAN).read())
20
21 # construct the testing image iterator
22 testIter = mx.io.ImageRecordIter(
23 path_imgrec=config.TEST_MX_REC,
24 data_shape=(3, 227, 227),
25 batch_size=config.BATCH_SIZE,
26 mean_r=means["R"],
27 mean_g=means["G"],
28 mean_b=means["B"])
30 # load the checkpoint from disk
31 print("[INFO] loading model...")
32 checkpointsPath = os.path.sep.join([args["checkpoints"],
33 args["prefix"]])
34 model = mx.model.FeedForward.load(checkpointsPath,
35 args["epoch"])
37 # compile the model
38 model = mx.model.FeedForward(
39 ctx=[mx.gpu(0)],
40 symbol=model.symbol,
41 arg_params=model.arg_params,
42 aux_params=model.aux_params)
44 # make predictions on the testing data
45 print("[INFO] predicting on test data...")
46 metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5)]
47 (rank1, rank5) = model.score(testIter, eval_metric=metrics)
48
49 # display the rank-1 and rank-5 accuracies
50 print("[INFO] rank-1: {:.2f}%".format(rank1 * 100))
51 print("[INFO] rank-5: {:.2f}%".format(rank5 * 100))


