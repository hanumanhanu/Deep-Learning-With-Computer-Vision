8.1.3 Implementing GoogLeNet

--- pyimagesearch
| |--- __init__.py
| |--- callbacks
| |--- nn
| | |--- __init__.py
| | |--- conv
| | |--- mxconv
| | | |--- __init__.py
| | | |--- mxalexnet.py
| | | |--- mxgooglenet.py
| | | |--- mxvggnet.py
| |--- preprocessing
| |--- utils

# import the necessary packages
import mxnet as mx
class MxGoogLeNet:
@staticmethod
def conv_module(data, K, kX, kY, pad=(0, 0), stride=(1, 1)):
# define the CONV => BN => RELU pattern
conv = mx.sym.Convolution(data=data, kernel=(kX, kY),um_filter=K, pad=pad, stride=stride)
bn = mx.sym.BatchNorm(data=conv)
act = mx.sym.Activation(data=bn, act_type="relu")
# return the block
return act
@staticmethod
def inception_module(data, num1x1, num3x3Reduce, num3x3,num5x5Reduce, num5x5, num1x1Proj):
# the first branch of the Inception module consists of 1x1
# convolutions
conv_1x1 = MxGoogLeNet.conv_module(data, num1x1, 1, 1)
# the second branch of the Inception module is a set of 1x1
# convolutions followed by 3x3 convolutions
conv_r3x3 = MxGoogLeNet.conv_module(data, num3x3Reduce, 1, 1)
conv_3x3 = MxGoogLeNet.conv_module(conv_r3x3, num3x3, 3, 3,pad=(1, 1))
# the third branch of the Inception module is a set of 1x1
# convolutions followed by 5x5 convolutions
conv_r5x5 = MxGoogLeNet.conv_module(data, num5x5Reduce, 1, 1)
conv_5x5 = MxGoogLeNet.conv_module(conv_r5x5, num5x5, 5, 5,pad=(2, 2))
# the final branch of the Inception module is the POOL +
# projection layer set
 pool = mx.sym.Pooling(data=data, pool_type="max", pad=(1, 1),kernel=(3, 3), stride=(1, 1))
conv_proj = MxGoogLeNet.conv_module(pool, num1x1Proj, 1, 1)
# concatenate the filters across the channel dimension
concat = mx.sym.Concat(*[conv_1x1, conv_3x3, conv_5x5,conv_proj])

# return the block
return concat
 @staticmethod
def build(classes):
# data input
data = mx.sym.Variable("data")
# Block #1: CONV => POOL => CONV => CONV => POOL
conv1_1 = MxGoogLeNet.conv_module(data, 64, 7, 7,pad=(3, 3), stride=(2, 2))
pool1 = mx.sym.Pooling(data=conv1_1, pool_type="max",pad=(1, 1), kernel=(3, 3), stride=(2, 2))
conv1_2 = MxGoogLeNet.conv_module(pool1, 64, 1, 1)
conv1_3 = MxGoogLeNet.conv_module(conv1_2, 192, 3, 3,ad=(1, 1))
pool2 = mx.sym.Pooling(data=conv1_3, pool_type="max",pad=(1, 1), kernel=(3, 3), stride=(2, 2))
# Block #3: (INCEP * 2) => POOL
in3a = MxGoogLeNet.inception_module(pool2, 64, 96, 128, 16,32, 32)
in3b = MxGoogLeNet.inception_module(in3a, 128, 128, 192, 32,96, 64)
pool3 = mx.sym.Pooling(data=in3b, pool_type="max",pad=(1, 1), kernel=(3, 3), stride=(2, 2))
# Block #4: (INCEP * 5) => POOL
in4a = MxGoogLeNet.inception_module(pool3, 192, 96, 208, 16,48, 64)
in4b = MxGoogLeNet.inception_module(in4a, 160, 112, 224, 24,
64, 64)
in4c = MxGoogLeNet.inception_module(in4b, 128, 128, 256, 24,
64, 64)
in4d = MxGoogLeNet.inception_module(in4c, 112, 144, 288, 32,64, 64)
in4e = MxGoogLeNet.inception_module(in4d, 256, 160, 320, 32,128, 128)
pool4 = mx.sym.Pooling(data=in4e, pool_type="max",
pad=(1, 1), kernel=(3, 3), stride=(2, 2))
# Block #5: (INCEP * 2) => POOL => DROPOUT
in5a = MxGoogLeNet.inception_module(pool4, 256, 160, 320, 32,128, 128)
in5b = MxGoogLeNet.inception_module(in5a, 384, 192, 384, 48,128, 128)
pool5 = mx.sym.Pooling(data=in5b, pool_type="avg",kernel=(7, 7), stride=(1, 1))
do = mx.sym.Dropout(data=pool5, p=0.4)
# softmax classifier
flatten = mx.sym.Flatten(data=do)
fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=classes)
model = mx.sym.SoftmaxOutput(data=fc1, name="softmax")
# return the network architecture
return model

8.1.4 Training GoogLeNet

--- mx_imagenet_googlenet
| |--- config
| | |--- __init__.py
| | |--- imagnet_googlenet_config.py
| |--- output/
| |--- test_googlenet.py
| |--- train_googlenet.py

# import the necessary packages
from config import imagenet_googlenet_config as config
from pyimagesearch.nn.mxconv import MxGoogLeNet
import mxnet as mx
import argparse 
import logging
import json
import os
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--checkpoints", required=True,
help="path to output checkpoint directory")
ap.add_argument("-p", "--prefix", required=True,
help="name of model prefix")
ap.add_argument("-s", "--start-epoch", type=int, default=0,
help="epoch to restart training at")
args = vars(ap.parse_args())
# set the logging level and output file
logging.basicConfig(level=logging.DEBUG,
filename="training_{}.log".format(args["start_epoch"]),filemode="w")
# load the RGB means for the training set, then determine the batch
# size
means = json.loads(open(config.DATASET_MEAN).read())
batchSize = config.BATCH_SIZE * config.NUM_DEVICES
# construct the training image iterator
trainIter = mx.io.ImageRecordIter(path_imgrec=config.TRAIN_MX_REC,data_shape=(3, 224, 224),batch_size=batchSize,rand_crop=True,rand_mirror=True,rotate=15,max_shear_ratio=0.1,mean_r=means["R"],mean_g=means["G"],mean_b=means["B"],preprocess_threads=config.NUM_DEVICES * 2)
 # construct the validation image iterator
valIter = mx.io.ImageRecordIter(path_imgrec=config.VAL_MX_REC,data_shape=(3, 224, 224),batch_size=batchSize,mean_r=means["R"],mean_g=means["G"],mean_b=means["B"])
# initialize the optimizer
opt = mx.optimizer.Adam(learning_rate=1e-3, wd=0.0002,rescale_grad=1.0 / batchSize)
# construct the checkpoints path, initialize the model argument and
# auxiliary parameters
checkpointsPath = os.path.sep.join([args["checkpoints"],args["prefix"]])
argParams = None
auxParams = None
# if there is no specific model starting epoch supplied, then
# initialize the network
if args["start_epoch"] <= 0:
# build the LeNet architecture
print("[INFO] building network...")
model = MxGoogLeNet.build(config.NUM_CLASSES)
# otherwise, a specific checkpoint was supplied
else:
# load the checkpoint from disk
print("[INFO] loading epoch {}...".format(args["start_epoch"]))
model = mx.model.FeedForward.load(checkpointsPath,args["start_epoch"])
# update the model and parameters
argParams = model.arg_params
auxParams = model.aux_params
model = model.symbol
 # compile the model
 model = mx.model.FeedForward(
ctx=[mx.gpu(0), mx.gpu(1), mx.gpu(2)],symbol=model,initializer=mx.initializer.Xavier(),arg_params=argParams,aux_params=auxParams,optimizer=opt,num_epoch=90,begin_epoch=args["start_epoch"])
# initialize the callbacks and evaluation metrics
batchEndCBs = [mx.callback.Speedometer(batchSize, 250)]
epochEndCBs = [mx.callback.do_checkpoint(checkpointsPath)]
metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5),mx.metric.CrossEntropy()]
# train the network
 print("[INFO] training network...")
 model.fit(X=trainIter,eval_data=valIter,eval_metric=metrics,batch_end_callback=batchEndCBs,epoch_end_callback=epochEndCBs)


